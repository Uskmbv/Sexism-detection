# -*- coding: utf-8 -*-
"""USKEMBAYEVA_ZIFLA_ML_METHODS_FINAL_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dJFt5krKDHjI4VyYFglQ-sEMuHcwPL61

# **Libraries**
"""

# Installing pandas and other necessary libraries
!pip install pandas
!pip install transformers==4.29.2 datasets scikit-learn accelerate==0.21.0 lime shap bertviz


# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import load_dataset, Dataset, DatasetDict
import torch
from torch.utils.data import DataLoader
import lime
from lime.lime_text import LimeTextExplainer
import shap
from sklearn.utils.class_weight import compute_class_weight

# Verifying installation
import transformers
import datasets
import sklearn
import accelerate

print(f"Pandas version: {pd.__version__}")
print(f"Transformers version: {transformers.__version__}")
print(f"Datasets version: {datasets.__version__}")
print(f"Scikit-learn version: {sklearn.__version__}")
print(f"Accelerate version: {accelerate.__version__}")

"""# **Dataset**"""

# Loading the "sexismreddit" dataset from Hugging Face
dataset = load_dataset("natural-lang-processing/sexismreddit")

# Converting the train split to a Pandas DataFrame
df_train = pd.DataFrame(dataset['train'])

# Displaying the first few rows to understand the structure
print(df_train.head())

# Inspecting the dataset structure
print(df_train.columns)
print(df_train['label_sexist'].unique())
print(df_train['label_category'].unique())

# Visualization of the target class distribution based on label_category
train_class_distribution = df_train['label_category'].value_counts()

# Plotting
fig, ax = plt.subplots()
train_class_distribution.plot(kind='bar', color=plt.cm.Paired.colors)
plt.xlabel('Types of Sexism')
plt.ylabel('Number of Sentences')
plt.title('Distribution of Types of Sexism in the Dataset')
plt.xticks(rotation=45, ha="right")
plt.show()

# Checking the distribution of 'label_sexist' in the train split
train_class_distribution = df_train['label_sexist'].value_counts()

# Calculating the minimum class count among all labels
min_class_count = min(train_class_distribution)

# Sampling the minimum class count from each class to balance the dataset
df_train_balanced = df_train.groupby('label_sexist').apply(lambda x: x.sample(min_class_count)).reset_index(drop=True)

# Checking the new distribution
train_class_distribution_balanced = df_train_balanced['label_sexist'].value_counts()
print(train_class_distribution_balanced)

"""# **Baseline model (Logistical regression)**"""

# Sampling 700 examples for training, 200 for validation, and 100 for testing
df_train_sampled = df_train_balanced.sample(n=700, random_state=42)
df_validation_sampled = df_train_balanced.sample(n=200, random_state=42)

# Creating a DatasetDict from the sampled data
dataset_dict = DatasetDict({
    'train': Dataset.from_pandas(df_train_sampled),
    'validation': Dataset.from_pandas(df_validation_sampled)
})

# Printing the unique values in label_category to ensure they match the label_map
print(df_train_sampled['label_category'].unique())

# Preparing data for the baseline model
X_train, y_train = df_train_sampled['text'], df_train_sampled['label_category']
X_val, y_val = df_validation_sampled['text'], df_validation_sampled['label_category']
X_test, y_test = df_test_sampled['text'], df_test_sampled['label_category']

# Tokenizing and train the baseline model
model_baseline = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))
model_baseline.fit(X_train, y_train)

# Evaluating the baseline model
y_pred_baseline = model_baseline.predict(X_test)
print("Baseline Model Evaluation:")
print(classification_report(y_test, y_pred_baseline))

from sklearn.model_selection import cross_val_predict

# Perform cross-validation
y_pred_cv = cross_val_predict(model_baseline, X_train, y_train, cv=5)

# Evaluating the baseline model with cross-validation
print("Baseline Model Cross-Validation Evaluation:")
print(classification_report(y_train, y_pred_cv))

from sklearn.utils.class_weight import compute_class_weight

# Computing class weights
class_weights = compute_class_weight(class_weight='balanced', classes=y_train.unique(), y=y_train)
class_weights_dict = {cls: weight for cls, weight in zip(y_train.unique(), class_weights)}

# Training the baseline model with class weights
model_baseline = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000, class_weight=class_weights_dict))
model_baseline.fit(X_train, y_train)

# Evaluating the baseline model
y_pred_baseline = model_baseline.predict(X_test)
print("Baseline Model Evaluation with Class Weights:")
print(classification_report(y_test, y_pred_baseline))

"""# **DistilBERT**

# **Re-loading dataset**
"""

# Loading the "sexismreddit" dataset from Hugging Face
dataset = load_dataset("natural-lang-processing/sexismreddit")

# Converting the train split to a Pandas DataFrame
df_train = pd.DataFrame(dataset['train'])

# Displaying the first few rows to understand the structure
print(df_train.head())

# Inspecting the dataset structure
print(df_train.columns)
print(df_train['label_sexist'].unique())
print(df_train['label_category'].unique())

# Checking the distribution of 'label_sexist' in the train split
train_class_distribution = df_train['label_sexist'].value_counts()

# Calculating the minimum class count among all labels
min_class_count = min(train_class_distribution)

# Sampling the minimum class count from each class to balance the dataset
df_train_balanced = df_train.groupby('label_sexist').apply(lambda x: x.sample(min_class_count)).reset_index(drop=True)

# Checking the new distribution
train_class_distribution_balanced = df_train_balanced['label_sexist'].value_counts()
print(train_class_distribution_balanced)

"""# **Tokenization**"""

# Sampling 900 examples for training and 250 examples for validation
df_train_sampled = df_train_balanced.sample(n=700, random_state=42)
df_validation_sampled = df_train_balanced.sample(n=200, random_state=42)

# Create a DatasetDict from the sampled data
dataset_dict = DatasetDict({
    'train': Dataset.from_pandas(df_train_sampled),
    'validation': Dataset.from_pandas(df_validation_sampled)
})

# Print the unique values in label_category to ensure they match the label_map
print(df_train_sampled['label_category'].unique())

# Tokenizer and Model Initialization
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=5)

# Function to process labels and tokenize text
def process_data(examples):
    label_map = {
        "none": 0,
        "3. animosity": 1,
        "2. derogation": 2,
        "4. prejudiced discussions": 3,
        "1. threats, plans to harm and incitement": 4
    }
    encodings = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)
    encodings['labels'] = [label_map[label] for label in examples["label_category"]]
    return encodings

# Applying processing function to the dataset
encoded_dataset = dataset_dict.map(process_data, batched=True)

# Converting to PyTorch tensors
encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Checking dataset sizes
print(f"Training examples: {len(encoded_dataset['train'])}")
print(f"Validation examples: {len(encoded_dataset['validation'])}")

### had problems with mapping labels categories

# DataLoader
train_dataloader = DataLoader(encoded_dataset['train'], batch_size=16, shuffle=True)
validation_dataloader = DataLoader(encoded_dataset['validation'], batch_size=16)

"""# **DistilBERT training**"""

# Training Arguments with Learning Rate Scheduler
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,  # Increased number of epochs from the first attempts
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=3e-5,  # Adjusted the learning rate as well
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    lr_scheduler_type="linear",  # Linear learning rate scheduler
)

# Metrics Function
def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    accuracy = accuracy_score(labels, pred)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted', zero_division=1)
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Trainer Initialization
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset['train'],
    eval_dataset=encoded_dataset['validation'],
    compute_metrics=compute_metrics
)

# Training the Model
trainer.train()

# Saving the trained model and tokenizer
model.save_pretrained('./trained_model')
tokenizer.save_pretrained('./trained_model')

# Loading the trained model and tokenizer
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Loading the trained model and tokenizer
model_directory = './trained_model'

tokenizer = AutoTokenizer.from_pretrained(model_directory)
model = AutoModelForSequenceClassification.from_pretrained(model_directory)

# Ensuring model is on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

df_test = pd.DataFrame(dataset['test'])

# Creating a DatasetDict from the test data
dataset_dict = DatasetDict({
    'test': Dataset.from_pandas(df_test)
})

# Function to process labels and tokenize text
def process_data(examples):
    label_map = {
        "none": 0,
        "3. animosity": 1,
        "2. derogation": 2,
        "4. prejudiced discussions": 3,
        "1. threats, plans to harm and incitement": 4
    }
    encodings = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)
    encodings['labels'] = [label_map[label] for label in examples["label_category"]]
    return encodings

# Applying processing function to the test dataset
encoded_test_dataset = dataset_dict.map(process_data, batched=True)

# Convert to PyTorch tensors
encoded_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Evaluate the model on the test set
trainer = Trainer(
    model=model,
    compute_metrics=compute_metrics
)

test_results = trainer.evaluate(encoded_test_dataset['test'])

print("Test Set Evaluation Results:")
print(test_results)

# Function to make predictions and format output
def classify_text(text, model, tokenizer):
    model.eval()  # Set the model to evaluation mode
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the device
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_class = torch.argmax(logits, dim=1).item()

    # Mapping for sexism detection
    sexism_map = {
        0: "This sentence is non-Sexist",
        1: "This sentence is Sexist",
        2: "This sentence is Sexist",
        3: "This sentence is Sexist",
        4: "This sentence is Sexist"
    }

    # Detailed mapping for type of sexism
    detailed_label_map = {
        0: "This sentence is non-Sexist",
        1: "This sentence exhibits animosity",
        2: "This sentence exhibits derogation",
        3: "This sentence involves prejudiced discussions",
        4: "This sentence contains threats, plans to harm and incitement"
    }

    sexism_result = sexism_map[predicted_class]
    detailed_result = detailed_label_map[predicted_class]

    print("\nInput Text:")
    print(text)
    print("\nClassification Result:")
    print(sexism_result)
    if predicted_class != 0:
        print("Type of Sexism:")
        print(detailed_result)

# Example usage
test_str = "Women are a waste of space."
classify_text(test_str, model, tokenizer)

"""# **RoBERTa**

# **Re-loading dataset**
"""

# Loading the "sexismreddit" dataset from Hugging Face
dataset = load_dataset("natural-lang-processing/sexismreddit")

# Converting the train split to a Pandas DataFrame
df_train = pd.DataFrame(dataset['train'])

# Displaying the first few rows to understand the structure
print(df_train.head())

# Checking the distribution of 'label_sexist' in the train split
train_class_distribution = df_train['label_sexist'].value_counts()

# Calculating the minimum class count among all labels
min_class_count = min(train_class_distribution)

# Sampling the minimum class count from each class to balance the dataset
df_train_balanced = df_train.groupby('label_sexist').apply(lambda x: x.sample(min_class_count)).reset_index(drop=True)

# Checking the new distribution
train_class_distribution_balanced = df_train_balanced['label_sexist'].value_counts()
print(train_class_distribution_balanced)

# Sampling 900 examples for training, 250 for validation
df_train_sampled = df_train_balanced.sample(n=900, random_state=42)
df_validation_sampled = df_train_balanced.sample(n=250, random_state=42)

"""# **Tokenization**"""

from transformers import RobertaTokenizer, RobertaForSequenceClassification

# Loading the RoBERTa tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

# Defining a function to process labels and tokenize text
def process_data(examples):
    label_map = {
        "none": 0,
        "3. animosity": 1,
        "2. derogation": 2,
        "4. prejudiced discussions": 3,
        "1. threats, plans to harm and incitement": 4
    }
    encodings = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)
    encodings['labels'] = [label_map[label] for label in examples["label_category"]]
    return encodings

# Converting the sampled DataFrames to Hugging Face Datasets
train_dataset = Dataset.from_pandas(df_train_sampled)
validation_dataset = Dataset.from_pandas(df_validation_sampled)

# Applying the processing function to the datasets
encoded_train_dataset = train_dataset.map(process_data, batched=True)
encoded_validation_dataset = validation_dataset.map(process_data, batched=True)
encoded_test_dataset = test_dataset.map(process_data, batched=True)

# Setting the format of the datasets to PyTorch tensors
encoded_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
encoded_validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
encoded_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""# **Training**"""

# Loading the RoBERTa model
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=5)

# Defining training arguments
training_args = TrainingArguments(
    output_dir='./roberta_results',
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=3e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./roberta_logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    lr_scheduler_type="linear",
)

# Defining metrics function
def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    accuracy = accuracy_score(labels, pred)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted', zero_division=1)
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Initializing the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_train_dataset,
    eval_dataset=encoded_validation_dataset,
    compute_metrics=compute_metrics
)

# Training the RoBERTa model
trainer.train()

# Save the trained model and tokenizer
model.save_pretrained('./trained_model_rob')
tokenizer.save_pretrained('./trained_model_rob')

# Loading the trained model and tokenizer
tokenizer = AutoTokenizer.from_pretrained('./trained_model_rob')
model = AutoModelForSequenceClassification.from_pretrained('./trained_model_rob')

df_test = pd.DataFrame(dataset['test'])

# Create a DatasetDict from the test data
dataset_dict = DatasetDict({
    'test': Dataset.from_pandas(df_test)
})

# Function to process labels and tokenize text
def process_data(examples):
    label_map = {
        "none": 0,
        "3. animosity": 1,
        "2. derogation": 2,
        "4. prejudiced discussions": 3,
        "1. threats, plans to harm and incitement": 4
    }
    encodings = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)
    encodings['labels'] = [label_map[label] for label in examples["label_category"]]
    return encodings

# Applying processing function to the test dataset
encoded_test_dataset = dataset_dict.map(process_data, batched=True)

# Converting to PyTorch tensors
encoded_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Metrics Function
def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    accuracy = accuracy_score(labels, pred)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted', zero_division=1)
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Evaluating the model on the test set
trainer = Trainer(
    model=model,
    compute_metrics=compute_metrics
)

test_results = trainer.evaluate(encoded_test_dataset['test'])

print("Test Set Evaluation Results:")
print(test_results)

# Function to make predictions and format output
def classify_text(text, model, tokenizer):
    model.eval()  # Setting the model to evaluation mode
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)  # Moving the model to the device
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {key: value.to(device) for key, value in inputs.items()}  # Moving inputs to the device
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_class = torch.argmax(logits, dim=1).item()

    # Mapping for sexism detection
    sexism_map = {
        0: "This sentence is non-Sexist",
        1: "This sentence is Sexist",
        2: "This sentence is Sexist",
        3: "This sentence is Sexist",
        4: "This sentence is Sexist"
    }

    # Detailed mapping for type of sexism
    detailed_label_map = {
        0: "This sentence is non-Sexist",
        1: "This sentence exhibits animosity",
        2: "This sentence exhibits derogation",
        3: "This sentence involves prejudiced discussions",
        4: "This sentence contains threats, plans to harm and incitement"
    }

    sexism_result = sexism_map[predicted_class]
    detailed_result = detailed_label_map[predicted_class]

    print("\nInput Text:")
    print(text)
    print("\nClassification Result:")
    print(sexism_result)
    if predicted_class != 0:
        print("Type of Sexism:")
        print(detailed_result)

# Example usage
test_str = "A woman is as dumb as a stick."
classify_text(test_str, model, tokenizer)

"""# **LIME**"""

### LIME Explanation
#This cell uses the LIME library to explain the model's predictions.

import torch
from lime.lime_text import LimeTextExplainer

# Setting the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Function to predict probabilities for LIME
def predict_proba(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs).logits
        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
    return probs

# Initializing the LIME explainer
explainer = LimeTextExplainer(class_names=["none", "animosity", "derogation", "prejudiced discussions", "threats, plans to harm and incitement"])

# Example text
test_str = "We should allow beating women up."

# Generating LIME explanation
exp = explainer.explain_instance(test_str, predict_proba, num_features=10)
exp.show_in_notebook(text=True)

from lime.lime_text import LimeTextExplainer
import matplotlib.pyplot as plt

# Setting the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Function to predict probabilities for LIME
def predict_proba(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs).logits
        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
    return probs

# Initializing the LIME explainer
explainer = LimeTextExplainer(class_names=["none", "animosity", "derogation", "prejudiced discussions", "threats, plans to harm and incitement"])

# Example text
test_str = "We should allow beating women up."

# Generating LIME explanation
exp = explainer.explain_instance(test_str, predict_proba, num_features=10)

# Getting the predicted class
predicted_proba = predict_proba([test_str])
predicted_class = predicted_proba.argmax(axis=1)[0]

# Custom function to plot the LIME explanation with the correct class names
def custom_plot(exp, class_names, prediction):
    fig, ax = plt.subplots(figsize=(10, 5))

    values = exp.as_list()
    values.reverse()  # reverse to have the highest contributing factor at the top

    bar_colors = ['green' if v[1] > 0 else 'red' for v in values]
    ax.barh([v[0] for v in values], [v[1] for v in values], color=bar_colors)
    ax.set_xlabel('Contribution to Prediction')
    ax.set_title(f'LIME Explanation for Class: {class_names[prediction]}')

    plt.show()

# Displaying the custom plot
custom_plot(exp, explainer.class_names, predicted_class)